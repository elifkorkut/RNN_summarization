geoffrey everest hinton cc frs frsc[11] (born 6 december 1947) is a british-canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. since 2013, he has divided his time working for google (google brain) and the university of toronto. in 2017, he co-founded and became the chief scientific advisor of the vector institute in toronto.[12][13]with david rumelhart and ronald j. williams, hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks,[14] although they were not the first to propose the approach.[15] hinton is viewed as a leading figure in the deep learning community.[16][17][18][19][20] the dramatic image-recognition milestone of the alexnet designed in collaboration with his students alex krizhevsky[21] and ilya sutskever for the imagenet challenge 2012[22] was a breakthrough in the field of computer vision.[23]hinton received the 2018 turing award, together with yoshua bengio and yann lecun, for their work on deep learning.[24] they are sometimes referred to as the godfathers of ai and godfathers of deep learning,[25][26] and have continued to give public talks together.[27] in machine learning, backpropagation (backprop,[1] bp) is a widely used algorithm for training feedforward neural networks. generalizations of backpropagation exist for other artificial neural networks (anns), and for functions generally. these classes of algorithms are all referred to generically as backpropagation.[2] in fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input’output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. this efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. the backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming.[3]the term backpropagation strictly refers only to the algorithm for computing the gradient, not how the gradient is used; however, the term is often used loosely to refer to the entire learning algorithm, including how the gradient is used, such as by stochastic gradient descent.[4] backpropagation generalizes the gradient computation in the delta rule, which is the single-layer version of backpropagation, and is in turn generalized by automatic differentiation, where backpropagation is a special case of reverse accumulation (or reverse mode).[5] the term backpropagation and its general use in neural networks was announced in rumelhart, hinton & williams (1986a), then elaborated and popularized in rumelhart, hinton & williams (1986b), but the technique was independently rediscovered many times, and had many predecessors dating to the 1960s; see § history.[6] a modern overview is given in the deep learning textbook by goodfellow, bengio & courville (2016).[7]in mathematics gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. the idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.gradient descent is generally attributed to cauchy, who first suggested it in 1847.[1] hadamard independently proposed a similar method in 1907.[2][3] its convergence properties for non-linear optimization problems were first studied by haskell curry in 1944,[4] with the method becoming increasingly well-studied and used in the following decades.[5][6]@summarygeoffrey everest hinton is a british - canadian cognitive psychologist and computer scientist. he has divided his time working for google ( google brain ) and the university of toronto. he was co - author of a highly cited paper published in 1986 that popularized backpropagation algorithm for training multi - layer neural networks. the backpropagation algorithm works by computing the gradient of the loss function with respect to each weight individually. this efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss, gradient descent, or variants such as stochastic gradient descent. iterating backward from the last layer to avoid redundant calculations. gradient descent is generally attributed to cauchy, who first suggested it in 1847. it is the first - order iterative optimization algorithm for finding a local minimum of a differentiable function. the procedure is then known as gradient ascent, because this is the direction of steepest descent.